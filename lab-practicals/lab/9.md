MPI – Collective Communication
Involve all processes in a communicator. These operations are often optimized internally.
• Broadcasting and Gathering
o MPI_Bcast, MPI_Gather, MPI_Gatherv, MPI_Scatter, MPI_Scatterv
• Reductions
o MPI_Reduce, MPI_Allreduce, MPI_Reduce_scatter
• Other Collectives
o MPI_Barrier (synchronization)
o MPI_Allgather, MPI_Allgatherv, MPI_Alltoall, MPI_Alltoallv

Lab Activity 1-The following sample code compares Gather/Scatter vs Gatherv/Scatterv
#include <stdio.h>
#include <mpi.h>

int main(int argc, char \*argv[]) {
int rank, size;
int data[10] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}; // total data in root
int recv_buffer[10]; // buffer large enough to hold all data at root
int recv_count;
int sendbuf[10]; // local buffer (may be more than needed)

    // Scatterv/Gatherv parameters
    int sendcounts[4] = {2, 3, 2, 3};   // how many elements each process receives/sends
    int displs[4]     = {0, 2, 5, 7};   // starting index for each process in data array

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (size != 4) {
        if (rank == 0)
            printf("Please run with 4 processes!\n");
        MPI_Finalize();
        return 0;
    }

    printf("=== Process %d ===\n", rank);

    // ---- Scatter vs Scatterv ----
    // With Scatter: each gets 2 elements
    int scatter_recv[2];
    MPI_Scatter(data, 2, MPI_INT, scatter_recv, 2, MPI_INT, 0, MPI_COMM_WORLD);

    printf("MPI_Scatter: received [");
    for (int i = 0; i < 2; i++) printf(" %d", scatter_recv[i]);
    printf(" ]\n");

    // With Scatterv: variable elements per process
    recv_count = sendcounts[rank];
    int scatterv_recv[3]; // max 3
    MPI_Scatterv(data, sendcounts, displs, MPI_INT, scatterv_recv, recv_count, MPI_INT, 0, MPI_COMM_WORLD);

    printf("MPI_Scatterv: received [");
    for (int i = 0; i < recv_count; i++) printf(" %d", scatterv_recv[i]);
    printf(" ]\n");

    // Modify received data to simulate processing
    for (int i = 0; i < recv_count; i++) {
        scatterv_recv[i] *= 10;
    }

    // ---- Gather vs Gatherv ----
    // With Gather: send 2 elements back
    int gather_send[2] = {rank*10, rank*10 + 1};
    MPI_Gather(gather_send, 2, MPI_INT, recv_buffer, 2, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("MPI_Gather: gathered [");
        for (int i = 0; i < 8; i++) printf(" %d", recv_buffer[i]);
        printf(" ]\n");
    }

    // With Gatherv: send variable elements back
    MPI_Gatherv(scatterv_recv, recv_count, MPI_INT,
                recv_buffer, sendcounts, displs, MPI_INT,
                0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("MPI_Gatherv: gathered [");
        for (int i = 0; i < 10; i++) printf(" %d", recv_buffer[i]);
        printf(" ]\n");
    }

    MPI_Finalize();
    return 0;

}

Lab Activity 2-The following compares MPI_Reduce, MPI_Allreduce, MPI_Reduce_scatter
#include <stdio.h>
#include <mpi.h>

int main(int argc, char \*argv[]) {
int rank, size;
int send_data[4]; // data each process contributes
int reduce_result[4]; // result of Reduce (at root)
int allreduce_result[4]; // result of Allreduce (on all processes)
int reduce_scatter_result; // each process gets one part

    int recvcounts[4] = {1, 1, 1, 1}; // 1 value per process for Reduce_scatter

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (size != 4) {
        if (rank == 0)
            printf("Please run with 4 processes!\n");
        MPI_Finalize();
        return 0;
    }

    // Each process fills in 4 elements: [rank+1, rank+2, rank+3, rank+4]
    for (int i = 0; i < 4; i++) {
        send_data[i] = rank + i + 1;
    }

    // --- MPI_Reduce ---
    MPI_Reduce(send_data, reduce_result, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("MPI_Reduce result at root: ");
        for (int i = 0; i < 4; i++) printf("%d ", reduce_result[i]);
        printf("\n");
    }

    // --- MPI_Allreduce ---
    MPI_Allreduce(send_data, allreduce_result, 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

    printf("Process %d - MPI_Allreduce result: ", rank);
    for (int i = 0; i < 4; i++) printf("%d ", allreduce_result[i]);
    printf("\n");

    // --- MPI_Reduce_scatter ---
    MPI_Reduce_scatter(send_data, &reduce_scatter_result, recvcounts, MPI_INT,
                       MPI_SUM, MPI_COMM_WORLD);

    printf("Process %d - MPI_Reduce_scatter result: %d\n", rank, reduce_scatter_result);

    MPI_Finalize();
    return 0;
}


Lab Activity 3 - Complete Example with MPI_Bcast and MPI_Barrier
#include <stdio.h>
#include <mpi.h>
#include <unistd.h>

int main(int argc, char *argv[]) {
    int rank, size;
    int broadcast_data = 0;
    
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    
    // Root process sets the data to broadcast
    if (rank == 0) {
        broadcast_data = 42;
        printf("Root process broadcasting value: %d\n", broadcast_data);
    }
    
    // Broadcast the data from root to all processes
    MPI_Bcast(&broadcast_data, 1, MPI_INT, 0, MPI_COMM_WORLD);
    
    printf("Process %d received broadcast value: %d\n", rank, broadcast_data);
    
    // Synchronize all processes before proceeding
    printf("Process %d reaching barrier...\n", rank);
    MPI_Barrier(MPI_COMM_WORLD);
    
    printf("Process %d passed the barrier!\n", rank);
    
    MPI_Finalize();
    return 0;
}

How to compile and run these examples:
1. Save each lab activity as separate .c files (e.g., lab1.c, lab2.c, lab3.c)
2. Compile with: mpicc -o lab1 lab1.c
3. Run with: mpirun -np 4 ./lab1

Expected outputs:
- Lab Activity 1: Shows difference between fixed-size scatter/gather vs variable-size scatterv/gatherv
- Lab Activity 2: Demonstrates reduction operations - Reduce (result only at root), Allreduce (result at all processes), Reduce_scatter (distributed result)
- Lab Activity 3: Shows broadcasting and synchronization with barriers
