Week 8 - Lab_Activity MPI.

1. Compare between OpenMP and MPI.
2. Suitable scenario/suggestions when each will be used.
3. Can we use a hybrid of OpenMp and MPI, if so, when?

Pls run the following example code and answer the following questions:
Example A:

```cpp
#include <stdio.h>
#include <omp.h>

int main() {
int i, sum = 0;
int array[100];

    // Initialize array
    for (i = 0; i < 100; i++) array[i] = i + 1;

    #pragma omp parallel for reduction(+:sum)
    for (i = 0; i < 100; i++) {
        sum += array[i];
    }

    printf("Total sum (OpenMP): %d\n", sum);
    return 0;

}
```

Example B
Pls find the necessary commands to compile and run the following MPI code:

```cpp
#include <stdio.h>
#include <mpi.h>

int main(int argc, char\* argv[]) {
int rank, size, i;
int array[100];
int local_sum = 0, total_sum = 0;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get current process rank
    MPI_Comm_size(MPI_COMM_WORLD, &size); // Get total number of processes

    // Only root process initializes the array
    if (rank == 0) {
        for (i = 0; i < 100; i++) array[i] = i + 1;
    }

    // Scatter 100/size elements to each process
    int chunk = 100 / size;
    int local_array[chunk];
    MPI_Scatter(array, chunk, MPI_INT, local_array, chunk, MPI_INT, 0, MPI_COMM_WORLD);

    // Each process computes partial sum
    for (i = 0; i < chunk; i++) {
        local_sum += local_array[i];
    }

    // Reduce all partial sums into total sum at root
    MPI_Reduce(&local_sum, &total_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Total sum (MPI): %d\n", total_sum);
    }

    MPI_Finalize();
    return 0;

}
```

Example C:

```cpp
#include <stdio.h>
#include <stdlib.h>
#include <omp.h>
#include <mpi.h>

int main(int argc, char* argv[]) {
int rank, size, i;
int array[100];
int chunk, *local_array;
int local_sum = 0, total_sum = 0;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    chunk = 100 / size;
    local_array = (int*)malloc(chunk * sizeof(int));

    // Initialize full array in root process
    if (rank == 0) {
        for (i = 0; i < 100; i++) array[i] = i + 1;
    }

    // Distribute data to all processes
    MPI_Scatter(array, chunk, MPI_INT, local_array, chunk, MPI_INT, 0, MPI_COMM_WORLD);

    // Parallel sum using OpenMP within each process
    #pragma omp parallel for reduction(+:local_sum)
    for (i = 0; i < chunk; i++) {
        local_sum += local_array[i];
    }

    // Combine all local sums into total sum at root process
    MPI_Reduce(&local_sum, &total_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Total sum (Hybrid MPI + OpenMP): %d\n", total_sum);
    }

    free(local_array);
    MPI_Finalize();
    return 0;

}
```

Questions:

1. Pls screenshot the results of each example code.
2. Pls understand and explain (during class), the code for the MPI version.
